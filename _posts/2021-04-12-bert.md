---
layout: article
title: NLP-Bert&Albert 모델 설명 및 한글 데이터셋 pre-training
aside:
 toc: true
tags: nlp
disqus: true
---

# Contents
* 모델 구조 설명
	* [bert](#bert)
	* [albert](#albert)
* 한글 데이터셋 사전 학습 
	* [tutorial](#한글-데이터셋을-이용한-사전-학습-tutorial)
* 한글 Bert 활용
	* [embedding](#application)

---
# Bert
Bert 모델의 2가지 대표적인 특징은 **transfer learning(전이학습)**을 한다는 점과 **transformer(auto-encoder) model**이라는 점이다.    

**transfer model**은 대용량 데이터를 깊은 네트워크로 오랜 시간 학습해 pre-training 모델을 생성하고, 그 모델의 마지막 레이어를 풀고자하는 task에 맞게 변형해 fine-tuning 하는 학습 순서를 따르는 모델을 의미한다. (참고: `ensemble`, `distillation`)   

**transformer**는 [Attention is all you need](https://arxiv.org/abs/1706.03762) 논문에서 제안한 네트워크 구조다. 기존 encoder-decoder모델은 학습이 한 방향으로 진행되는 *auto-regressive*한 RNN 구조로 이루어졌다. 이와 달리 transformer는 입력 sequence의 모든 토큰을 동시에 학습하는 *attention* 구조로만 이루어진 encoder-decoder이다.   

<div style="width:70%; margin:0 auto;" align="center" markdown="1">
![](/assets/transformer.png)
</div>>

Bert는 transformer의 encoder만 사용한다. Transformer의 encoder 구조는 다음과 같다. 

* Stack of layers (N=6)
	* Sub-layer1: Multi-head self-attention
		* residual connection
		* layer normalization
	* Sub-layer2: Fully-connected Feed-forword
		* residual connection
		* layer normalization

모든 layer의 input, output dimension은 동일(논문에서는 512)하게 유지된다. 

Bert 모델의 파라미터는 다음과 같다.   

| BERT Model | # layers | dimension (=hidden size) | # attention heads | total # params | 
| :---: | :---: | :---: | :---: | :---: |
| BERT (base) | 12 | 768 | 12 | 110M |
| BERT (large) | 24 | 1024 | 16 | 340M |


---
# Albert

Albert는 기존 Bert 모델의 2가지 단점을 개선했다. 먼저 모델의 파라미터 수를 줄이고 layer간에 파라미터를 공유하도록 구조를 변형해 **모델 크기**를 줄였다. 또한 Pre-training의 task 중 하나였던 Next Sentence Prediction의 학습 방법을 바꿔 문장 사이의 관계 학습하는 task (ex. MNLI)에서 기존 보다 높은 성능을 보였다.    

Bert(base) 모델의 parameter 개수는 1억개다. 만약 12G 용량의 GPU 1대가 있다고 가정했을 때, Out-of-memory에러가 나지 않는 최대 batch size는 다음과 같다. 

| BERT Model | batch size | sequence length | 
| :---: | :---: | :---: |
| BERT (base) | 64 | 64 |
| BERT (base) | 32 | 128 |
| BERT (base) | 16 | 256 |
| BERT (base) | 14 | 320 |
| BERT (base) | 12 | 384 |
| BERT (base) | 6 | 512 |

보통 sequence length (input으로 입력되는 문장의 최대 token 개수)가 128이라고 가정해도 batch size 32는 굉장히 작다고 할 수 있다.   
따라서 일반적인 하드웨어 자원으로는 Bert 모델을 완벽하게 사용하기 힘들다는 것을 알 수 있다. 

---
# 한글 데이터셋을 이용한 사전 학습 Tutorial

### tokenization

### run pretrain using package 

### pretraining time & resource & result

### fine-tuning

---
# Application 

### token embedding


---
# Reference
[BERT official github](https://github.com/google-research/bert)
[BERT word encoding tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)   
[BERT encoder decoder dissect](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3)   
[BERT architecture parameters calculated](https://medium.com/analytics-vidhya/understanding-bert-architecture-3f35a264b187)   
[BERT 한글 설명: 톺아보기](http://docs.likejazz.com/bert/)   
---
layout: article
title: NLP-Bert&Albert 모델 설명 및 한글 데이터셋 pre-training
aside:
 toc: true
tags: nlp
disqus: true
---

# Contents
* 모델 구조 설명
	* [bert](#bert)
	* [albert](#albert)
* 한글 데이터셋 사전 학습 
	* [tutorial](#한글-데이터셋을-이용한-사전-학습-tutorial)
* 한글 Bert 활용
	* [embedding](#application)

---
# Bert

> [용어] Bert는 입력된 문장을 단어(word)로 구분하지 않는다. 대신 Wordpiece tokenizer로 생성된 subword로 구분한다. 예를 들어 "이 원피스 예뻐요."라는 문장이 입력됐을 때 wordpiece tokenizer는 ["이", "원피스", "예뻐", "#요"]라는 subword를 생성하며 각 subword가 Bert 모델의 인풋으로 들어간다. 따라서 이 포스팅에서는 모델의 인풋을 "단어"라는 용어 대신 `subword`라고 칭하겠다. 


Bert 모델의 2가지 대표적인 특징은 **transfer learning(전이학습)**을 한다는 점과 **transformer(auto-encoder) model**이라는 점이다.    

**transfer model**은 pre-training과 fine-tuning 단계가 있다. Pre-training은 인풋 sequence에서 mask된 subword를 찾는 ([language model](https://en.wikipedia.org/wiki/Language_model)) semi-supervised 학습을 대용량 데이터와 깊은 네트워크로 오랜 시간 학습해 학습한다. Fine-tuning은 pre-trained 모델의 마지막 레이어를 풀고자하는 task에 맞게 변형해 supervised 학습하는 모델이다. (참고: `ensemble`, `distillation`)   

**transformer**는 [Attention is all you need](https://arxiv.org/abs/1706.03762) 논문에서 제안한 네트워크 구조다. 기존 encoder-decoder모델은 학습이 한 방향으로 진행되는 *auto-regressive*한 RNN 구조로 이루어졌다. 이와 달리 transformer는 입력 sequence의 모든 토큰을 동시에 학습하는 *attention* 구조로만 이루어진 encoder-decoder이다.   

<div style="width:70%; margin:0 auto;" align="center" markdown="1">
![](/assets/transformer.png)
</div>

Bert는 transformer의 encoder를 여려겹 쌓아올린(stacked) 모델이다. Transformer의 encoder 구조는 다음과 같다. 

* Stack of layers (N=6)
	* Sub-layer1: `Multi-head self-attention`
		* residual connection
		* layer normalization
	* Sub-layer2: `Fully-connected Feed-forword`
		* residual connection
		* layer normalization


Bert 모델의 파라미터는 다음과 같다.   

| BERT Model | # layers | dimension (=hidden size) | # attention heads | total # params | 
| :---: | :---: | :---: | :---: | :---: |
| BERT (base) | 12 | 768 | 12 | 110M |
| BERT (large) | 24 | 1024 | 16 | 340M |

* number of layers
	* encoder layer의 개수
* dimension (hidden size)
	* subword embedding(vector)의 차원 
	* 모든 layer의 input, output dimension은 동일하게 유지된다. 
* max sequence len
	* 입력 sequence의 최대 subword 개수
	* 보통 training data의 가장 긴 문장의 subword를 개수로 설정한다. 
* number of attention heads
	* attention의 head 개수. multi-head attention은 head 개수만큼의 attention이 병렬 처리된다. 


<div style="width:100%; margin:0 auto;" align="center" markdown="1">
![](/assets/bert1.png)
</div>


그렇다면 Bert 모델의 핵심인 attention이란 무엇일까? 즉, `multi-head self-attention` layer에서는 어떤 값이 계산되는 것일까?   

Attention이 등장하기 이전까지 [seq-to-seq 모델](https://en.wikipedia.org/wiki/Seq2seq)에는 RNN, LSTM과 같은 recurrent 모델이 사용됐다. 하지만 recurrent 모델은 긴 문장이 인풋으로 들어올 때 hidden state(weight)값이 제대로 학습되지 않는다는 문제가 있다. ~~예를 들어 "뽀로로는 노는게 제일 좋아"라는 문장이 있을 때, 첫번째 단어인 "뽀로로"의 weight(w0)가 마지막 단어인 "좋아"의 weight(w3)를 계산하는데 거의 영향을 미치지 않는 문제가 발생한다.(수정 필요)~~    

Attention은 모든 input 단어들의 관련도 점수(attention score)를 계산하여 관련도 점수가 높은 단어는 많이, 점수가 낮은 단어는 적게 고려한다. 즉, output과 **관련된 input의 weight만 고려**한다.   

서두에 Transformer는 recurrent 모델을 사용하지 않고 attention으로만 이루어진 encoder-decoder(=seq2seq) 모델이라고 설명했다. Transformer의 또 다른 큰 특징은 attention이 행렬 연산이기 때문에 **병렬 처리** 학습이 가능하다는 점이다. 


Attention은 4가지 과정을 거쳐 계산된다. 
1. subword별로 Query, Key, Value 백터 생성
2. subword간(pair-wise)의 Score 계산
3. Score를 normalize(scaling) 함
4. Score에 softmax 함수를 적용해 [0,1] 사이의 확률 값으로 만듦
5. 모든 subword에 대해 4번 값과 Value 백터 값을 곱한다. 하나의 attention 값을 만들기 위해 subword의 score을 모두 더한다.  

 사용되는 벡터의 종류는 총 3가지이다.
* Q (query)
* K (key)
* V (value)

여기서 관련도 점수(attention score)는 Q와 K의 값으로 구한다.    
Attention score는 "Scaled dot product of Q,K"라고 하는데 이 점수를 [softmax](https://en.wikipedia.org/wiki/Softmax_function)함수에 넣어 0과 1사이의 확률값으로 만든다. 이 attention score는 V(value)의 weight 값으로 사용되 V의 가중치를 결정한다.    

<div style="width:50%; margin:0 auto;" align="center" markdown="1">
![](/assets/bert2.png)
</div>

(...TODO: more about attention...)


---
# Albert

Albert는 기존 Bert 모델의 2가지 단점을 개선했다. 먼저 모델의 파라미터 수를 줄이고 layer간에 파라미터를 공유하도록 구조를 변형해 **모델 크기를 줄였다.** 또한 Pre-training의 task 중 하나였던 **Next Sentence Prediction의 학습 방법을 바꿔 문장 사이의 관계 학습하는 task (ex. MNLI)에서 기존 보다 높은 성능**을 보였다.    

Bert(base) 모델의 parameter 개수는 1억개다. 만약 12G 용량의 GPU 1대가 있다고 가정했을 때, Out-of-memory에러가 나지 않는 최대 batch size는 다음과 같다. 

| BERT Model | batch size | sequence length | 
| :---: | :---: | :---: |
| BERT (base) | 64 | 64 |
| BERT (base) | 32 | 128 |
| BERT (base) | 16 | 256 |
| BERT (base) | 14 | 320 |
| BERT (base) | 12 | 384 |
| BERT (base) | **6** | **512** |

Bert(base) 모델의 sequence length가 512인데 GPU 1대로 fine-tuning을 한다고 할 때 batch size는 최대 6밖에 설정할 수 없다.       
따라서 일반적인 하드웨어 자원으로는 Bert 모델을 완벽하게 사용하기 힘들다. 

---
# 한글 데이터셋을 이용한 사전 학습 Tutorial

### tokenization

### run pretrain using package 

### pretraining time & resource & result

### fine-tuning

---
# Application 

### word embedding


---
# Reference
[BERT official github](https://github.com/google-research/bert)   
[Illustrated Bert](http://jalammar.github.io/illustrated-bert/)    
[Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)   
[Illustrated Seq2Seq model with attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)      
[BERT word encoding tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)   
[BERT encoder decoder dissect](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3)   
[BERT architecture parameters calculated](https://medium.com/analytics-vidhya/understanding-bert-architecture-3f35a264b187)      
[BERT 한글 설명: 톺아보기](http://docs.likejazz.com/bert/)   